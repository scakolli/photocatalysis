{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziploc = '/home/btpq/bt308495/Thesis/molecular-vae/data/processed.zip'\n",
    "contentsdest = '/localdisk/bt308495/molecular-vae/data/'\n",
    "\n",
    "### Unzip file to 'contentdest'\n",
    "# with zipfile.ZipFile(ziploc, 'r') as zpf:\n",
    "#     zpf.extractall(contentsdest)\n",
    "\n",
    "### Load data from unzipped file\n",
    "with h5py.File(os.path.join(contentsdest, 'processed.h5'), 'r') as data:\n",
    "    data_train =  data['data_train'][:]\n",
    "    data_test =  data['data_test'][:]\n",
    "    charset =  data['charset'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 120, 33), (10000, 120, 33), (33,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape, charset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_train[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_smile(onehot_vector, character_set):\n",
    "    ### Take a one-hot vector/tensor (MAX SMILE LENGTH, CHARSET LENGTH) and convert it to a smile string\n",
    "    assert onehot_vector.shape[1] == character_set.size, 'Onehot length doesnt match character_set length'\n",
    "    indicies = np.argmax(onehot_vector, axis=1)\n",
    "    return b''.join(character_set[indicies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f039d24e420>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmiles(one_hot_to_smile(data_train[30], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis = [one_hot_to_smile(d, charset) for d in data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.CHARSET_LEN = 33\n",
    "        self.INPUT_SIZE = 120\n",
    "        self.LATENT_DIM = 292\n",
    "\n",
    "        ### ENCODING\n",
    "        # Convolutional Layers\n",
    "        self.conv_1 = nn.Conv1d(self.INPUT_SIZE, 9, kernel_size=9)\n",
    "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.linear_0 = nn.Linear(70, 435)\n",
    "\n",
    "        # Mean and Variance Latent Layers\n",
    "        self.mean_linear_1 = nn.Linear(435, self.LATENT_DIM)\n",
    "        self.var_linear_2 = nn.Linear(435, self.LATENT_DIM)\n",
    "        \n",
    "        ### DECODING\n",
    "        # Fully connected, GRU RNN, Fully connected layers\n",
    "        # 3 sequential GRUs of hidden size 501. batch_first = True implies batch_dim first. \n",
    "        # Then, inputs into GRU are of shape [batch_size, seq_length (INPUT_SIZE, 120), Hin (LATENT_DIM, 292)]\n",
    "        self.linear_3 = nn.Linear(self.LATENT_DIM, self.LATENT_DIM)\n",
    "        self.stacked_gru = nn.GRU(self.LATENT_DIM, 501, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(501, self.CHARSET_LEN)\n",
    "        \n",
    "        ### ACTIVATION and OUTPUT \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Convolutional\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "\n",
    "        # Flatten the Convultional output [batch_size, 10, 70] to make an input [batch_size, 10*7] for a fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "\n",
    "        # Mean and logvariance latent vectors [batch_size, latent_dim]\n",
    "        m, v = self.mean_linear_1(x), self.var_linear_2(x) \n",
    "        return m, v\n",
    "\n",
    "    def reparameterize(self, mu_z, logvar_z):\n",
    "        ## Sample a latent vector 'z', given its mean and std vectors\n",
    "        # z ~ N(mu, std), is non-differentiable. While z ~ mu + eps (dot) std, where eps ~ N(0, 1), is differentiable. Why?\n",
    "        # Since mu and std are now deterministic model outputs that can be trained by backprop, while the 'randomness' implicitly enters via the standard normal error/epsilon term\n",
    "        gamma = 1e-2 # not sure why this is here...?\n",
    "        epsilon = gamma * torch.randn_like(logvar_z) # 0 mean, unit variance noise of shape z_logvar\n",
    "        std = torch.exp(0.5 * logvar_z)\n",
    "        z = mu_z + epsilon * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "\n",
    "        # Since the GRU, when unrolled in 'time', consists of 120 NNs each sequentially processing data... we have to send 120 copies through it.\n",
    "        # By repeating the tensor z self.INPUT_SIZE times along the sequence length dimension, we are effectively creating a sequence of self.INPUT_SIZE time steps,\n",
    "        # each with the same latent representation. This setup allows the GRU to process this \"sequence\" of repeated tensors, even though the actual sequence content\n",
    "        # is the same at each time step. This kind of setup can be useful for example when:\n",
    "\n",
    "        # 1. Information Propagation: \n",
    "        # Sometimes you want to ensure that a certain piece of information is propagated consistently through the entire sequence. By using repeated tensors, you can\n",
    "        # ensure that the same information is available to the network at every time step, allowing the network to incorporate this information throughout the entire sequence.\n",
    "\n",
    "        # 2. Fixed-Size Context: If you have a fixed-size context or control signal that should influence the processing of the entire sequence, you can repeat this\n",
    "        # context along the sequence length dimension. This way, the network can take into account this context when making decisions at every time step.\n",
    "\n",
    "        # Note on use of contiguous()\n",
    "        # contiguous means 'sharing a common border; touching'\n",
    "        # In the context of pytorch, contiguous means not only contiguous in memory (each element in a tensor is stored right next to the other, in a block),\n",
    "        # but also in the same order in memory as the indices order. For example doing a transposition doesn't change the data in memory (data at (1, 4) doesnt swap\n",
    "        # memory places when its transposed to (4, 1)), it simply changes the map from indices to memory pointers (what index corresponds to what data is swapped instead,\n",
    "        # leaving memory untouched). If you then apply contiguous() it will change the data in memory so that the map from indices to memory location is the canonical one.\n",
    "        # For certain pytorch operations, contiguously stored tensors are required! Else a runtime error is encountered (RuntimeError: input is not contiguous).\n",
    "\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, self.INPUT_SIZE, 1) # Reshape z from [batch_size, latent_dim] to [batch_size, seq_len (120), latent_dim]\n",
    "        output, hs = self.stacked_gru(z) # hs represents the hidden state of the last time step of the GRU\n",
    "\n",
    "        # Output is flattened along 1st two dimensions [batch_size, seq_len, hout] -> [batch_size * seq_len, hout]\n",
    "        # Softmax is then applied row-wise/sample-wise following a linear transform\n",
    "        # before the vector is then unflatten back to the original [batch_size, seq_len, charset_len]\n",
    "\n",
    "        # The purpose of this initial flattening is:\n",
    "        # In the context of a sequence-to-sequence model, each time step's output from the RNN (or a similar sequential model) represents the model's understanding of the\n",
    "        # data at that particular moment. When you collapse the dimensions and reshape the tensor to (batch_size * sequence_length, num_features), you effectively create \n",
    "        # a flat sequence where each element corresponds to a time step's output for a specific sample in the batch.\n",
    "        # Then applying a linear transformation like self.linear_4 at this stage means that the same linear transformation is applied to each element in the flattened sequence\n",
    "        # independently (as if the new batch size is of shape batch_size * seq_len)! This is independent in the sense that the transformation doesn't consider interactions\n",
    "        # between different time steps or different samples within the batch. It's a per-element operation.\n",
    "\n",
    "        # By applying a linear transformation independently to each element, the model has the flexibility to learn different weights for different features at different time steps.\n",
    "        # These weights can capture complex relationships within each time step's output, such as identifying important features or capturing patterns specific to that moment.\n",
    "        # We then reshape back to regain the sequence structure...\n",
    "        out_independent = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_independent), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_z, logvar_z = self.encode(x)\n",
    "        z = self.reparameterize(mu_z, logvar_z)\n",
    "        zhat = self.decode(z)\n",
    "        return zhat, mu_z, logvar_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 120, 33]              --\n",
       "├─Conv1d: 1-1                            [1, 9, 25]                9,729\n",
       "├─ReLU: 1-2                              [1, 9, 25]                --\n",
       "├─Conv1d: 1-3                            [1, 9, 17]                738\n",
       "├─ReLU: 1-4                              [1, 9, 17]                --\n",
       "├─Conv1d: 1-5                            [1, 10, 7]                1,000\n",
       "├─ReLU: 1-6                              [1, 10, 7]                --\n",
       "├─Linear: 1-7                            [1, 435]                  30,885\n",
       "├─Linear: 1-8                            [1, 292]                  127,312\n",
       "├─Linear: 1-9                            [1, 292]                  127,312\n",
       "├─Linear: 1-10                           [1, 292]                  85,556\n",
       "├─GRU: 1-11                              [1, 120, 501]             4,212,909\n",
       "├─Linear: 1-12                           [120, 33]                 16,566\n",
       "==========================================================================================\n",
       "Total params: 4,612,007\n",
       "Trainable params: 4,612,007\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 508.17\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.53\n",
       "Params size (MB): 18.45\n",
       "Estimated Total Size (MB): 18.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "torchinfo.summary(VAE(), input_size=(batch_size, 120, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_z = torch.from_numpy(np.array((np.random.randn(292), np.random.randn(292)))).to(torch.float32)\n",
    "# test_model = VAE()\n",
    "# test_model.decode(test_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_tensor[0].size(1)\n",
    "# data_train_tensor[:2].shape, data_train_tensor[:2].view(data_train_tensor[:2].shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_tensor = torch.from_numpy(data_train)\n",
    "data_train_tensor_loader = torch.utils.data.TensorDataset(torch.from_numpy(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0293, 0.0307, 0.0297,  ..., 0.0300, 0.0297, 0.0295],\n",
       "          [0.0290, 0.0305, 0.0299,  ..., 0.0298, 0.0298, 0.0292],\n",
       "          [0.0288, 0.0304, 0.0299,  ..., 0.0297, 0.0299, 0.0291],\n",
       "          ...,\n",
       "          [0.0284, 0.0301, 0.0298,  ..., 0.0295, 0.0300, 0.0289],\n",
       "          [0.0284, 0.0301, 0.0298,  ..., 0.0295, 0.0300, 0.0289],\n",
       "          [0.0284, 0.0301, 0.0298,  ..., 0.0295, 0.0300, 0.0289]],\n",
       " \n",
       "         [[0.0293, 0.0307, 0.0297,  ..., 0.0300, 0.0297, 0.0295],\n",
       "          [0.0290, 0.0305, 0.0299,  ..., 0.0298, 0.0299, 0.0292],\n",
       "          [0.0288, 0.0304, 0.0299,  ..., 0.0297, 0.0299, 0.0291],\n",
       "          ...,\n",
       "          [0.0285, 0.0301, 0.0297,  ..., 0.0295, 0.0300, 0.0289],\n",
       "          [0.0285, 0.0301, 0.0297,  ..., 0.0295, 0.0300, 0.0289],\n",
       "          [0.0285, 0.0301, 0.0297,  ..., 0.0295, 0.0300, 0.0289]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[-0.0247,  0.0105, -0.0620,  0.0238, -0.0278,  0.0079,  0.0418, -0.0597,\n",
       "          -0.0270, -0.0361, -0.1300,  0.1268,  0.0400, -0.0216,  0.0110, -0.0632,\n",
       "           0.0446, -0.0114, -0.0228,  0.0181,  0.0712, -0.0149, -0.0216,  0.0447,\n",
       "           0.0661,  0.0576, -0.0800, -0.0793, -0.0448,  0.0240,  0.0377, -0.0366,\n",
       "          -0.1446,  0.1246,  0.0083,  0.0444, -0.2119,  0.0524, -0.1168, -0.0146,\n",
       "           0.0047,  0.0536,  0.0541, -0.0249, -0.0122, -0.0161,  0.0083, -0.0728,\n",
       "          -0.0166,  0.0375, -0.0364,  0.0655,  0.0255, -0.0554,  0.0278,  0.0555,\n",
       "           0.0271,  0.0552,  0.0389, -0.1432, -0.0627,  0.0289,  0.0092, -0.0332,\n",
       "          -0.0833, -0.0976,  0.0210, -0.0269, -0.0038, -0.0243,  0.0375,  0.0247,\n",
       "          -0.0022,  0.0189, -0.0414, -0.1002,  0.0269,  0.0407,  0.0795, -0.0524,\n",
       "           0.0630,  0.0327,  0.0367,  0.0506,  0.0029,  0.0636,  0.0524, -0.0513,\n",
       "           0.0132,  0.0550,  0.0457,  0.0716,  0.0544, -0.1007, -0.1550, -0.0077,\n",
       "           0.0661, -0.0066, -0.0508, -0.0606, -0.0443,  0.0580, -0.0439, -0.0702,\n",
       "           0.0164,  0.0449,  0.0496, -0.0371,  0.0162,  0.0537, -0.1102,  0.0136,\n",
       "           0.0210, -0.0036,  0.0528, -0.0386, -0.0218, -0.0303, -0.1466, -0.0986,\n",
       "           0.0557,  0.0336, -0.0432, -0.0708,  0.0040,  0.0359, -0.0129, -0.0374,\n",
       "          -0.0722, -0.0735, -0.0142, -0.0411,  0.0885, -0.1160, -0.1565, -0.0170,\n",
       "          -0.0518,  0.0910, -0.0053,  0.0212, -0.0006,  0.0606,  0.0911,  0.0077,\n",
       "          -0.0124, -0.0341,  0.0270, -0.0210, -0.0231, -0.0080,  0.1315,  0.0085,\n",
       "          -0.0299,  0.0063,  0.0296,  0.0188,  0.0355, -0.0733, -0.0346,  0.0366,\n",
       "          -0.0347,  0.0599, -0.0144, -0.1029,  0.0519, -0.0448,  0.0611,  0.1752,\n",
       "           0.0137, -0.0536, -0.0582, -0.0232,  0.0492,  0.0767,  0.0776,  0.0489,\n",
       "           0.0162,  0.0291, -0.0575,  0.0562,  0.0216, -0.0165, -0.0244,  0.0578,\n",
       "          -0.0165,  0.1065, -0.1236,  0.0864,  0.0105, -0.0453,  0.0329,  0.0246,\n",
       "           0.0666,  0.0115,  0.0338,  0.0978, -0.1102, -0.0740,  0.0303,  0.0025,\n",
       "          -0.1036,  0.0249, -0.0366, -0.0255, -0.0970,  0.0723,  0.0784, -0.0226,\n",
       "          -0.0087,  0.0916,  0.0130,  0.0234,  0.1239, -0.0197, -0.0449, -0.0800,\n",
       "          -0.0304,  0.0600,  0.1337, -0.0099, -0.0342, -0.0009, -0.0221,  0.0107,\n",
       "          -0.0016, -0.0592,  0.0160, -0.0027,  0.0087, -0.0767,  0.0188,  0.0575,\n",
       "          -0.0102,  0.1348, -0.0469,  0.0659, -0.0375, -0.0155, -0.0140,  0.1280,\n",
       "           0.0692, -0.0654,  0.0866,  0.0296,  0.0205, -0.0999,  0.0306,  0.0959,\n",
       "           0.0318,  0.0316, -0.0538, -0.0791,  0.0888,  0.0149,  0.0987, -0.0709,\n",
       "           0.0961,  0.0169,  0.0432, -0.0644,  0.0303, -0.0059,  0.0127,  0.1292,\n",
       "          -0.0520, -0.0328,  0.0304, -0.0378,  0.0033, -0.0314, -0.0737, -0.0783,\n",
       "          -0.0048,  0.0670, -0.0555, -0.0110,  0.0356,  0.0313, -0.0056, -0.0301,\n",
       "          -0.0149, -0.0482,  0.0285,  0.0321, -0.0158, -0.1086, -0.0170, -0.0450,\n",
       "           0.0189,  0.0961, -0.0431, -0.0464],\n",
       "         [-0.0211,  0.0121, -0.0611,  0.0242, -0.0280,  0.0078,  0.0400, -0.0593,\n",
       "          -0.0262, -0.0362, -0.1297,  0.1324,  0.0421, -0.0241,  0.0088, -0.0603,\n",
       "           0.0459, -0.0103, -0.0249,  0.0198,  0.0724, -0.0145, -0.0165,  0.0455,\n",
       "           0.0680,  0.0542, -0.0805, -0.0798, -0.0446,  0.0207,  0.0352, -0.0345,\n",
       "          -0.1406,  0.1249,  0.0091,  0.0435, -0.2106,  0.0569, -0.1172, -0.0175,\n",
       "           0.0083,  0.0570,  0.0528, -0.0279, -0.0108, -0.0153,  0.0071, -0.0736,\n",
       "          -0.0162,  0.0406, -0.0318,  0.0656,  0.0259, -0.0560,  0.0265,  0.0569,\n",
       "           0.0282,  0.0530,  0.0362, -0.1486, -0.0621,  0.0274,  0.0092, -0.0319,\n",
       "          -0.0831, -0.1005,  0.0168, -0.0236, -0.0062, -0.0243,  0.0370,  0.0239,\n",
       "          -0.0034,  0.0193, -0.0404, -0.0962,  0.0262,  0.0409,  0.0817, -0.0540,\n",
       "           0.0641,  0.0300,  0.0340,  0.0506,  0.0041,  0.0651,  0.0496, -0.0508,\n",
       "           0.0127,  0.0566,  0.0463,  0.0748,  0.0562, -0.0994, -0.1528, -0.0067,\n",
       "           0.0667, -0.0094, -0.0509, -0.0597, -0.0455,  0.0609, -0.0435, -0.0703,\n",
       "           0.0182,  0.0449,  0.0496, -0.0361,  0.0172,  0.0547, -0.1098,  0.0158,\n",
       "           0.0223, -0.0048,  0.0498, -0.0387, -0.0215, -0.0299, -0.1489, -0.1001,\n",
       "           0.0541,  0.0332, -0.0417, -0.0730,  0.0014,  0.0364, -0.0094, -0.0385,\n",
       "          -0.0718, -0.0734, -0.0170, -0.0407,  0.0873, -0.1158, -0.1573, -0.0182,\n",
       "          -0.0519,  0.0890, -0.0043,  0.0215,  0.0022,  0.0602,  0.0928,  0.0050,\n",
       "          -0.0146, -0.0365,  0.0294, -0.0183, -0.0240, -0.0105,  0.1319,  0.0063,\n",
       "          -0.0263,  0.0027,  0.0324,  0.0167,  0.0340, -0.0717, -0.0359,  0.0371,\n",
       "          -0.0350,  0.0578, -0.0143, -0.1020,  0.0532, -0.0430,  0.0599,  0.1732,\n",
       "           0.0132, -0.0533, -0.0584, -0.0226,  0.0499,  0.0765,  0.0780,  0.0493,\n",
       "           0.0147,  0.0272, -0.0588,  0.0553,  0.0235, -0.0167, -0.0254,  0.0561,\n",
       "          -0.0152,  0.1077, -0.1220,  0.0885,  0.0141, -0.0450,  0.0335,  0.0233,\n",
       "           0.0638,  0.0112,  0.0347,  0.0976, -0.1118, -0.0716,  0.0291,  0.0022,\n",
       "          -0.1068,  0.0272, -0.0356, -0.0237, -0.0962,  0.0747,  0.0848, -0.0234,\n",
       "          -0.0148,  0.0877,  0.0127,  0.0218,  0.1232, -0.0191, -0.0429, -0.0797,\n",
       "          -0.0291,  0.0604,  0.1366, -0.0098, -0.0378, -0.0032, -0.0231,  0.0104,\n",
       "          -0.0008, -0.0593,  0.0151, -0.0015,  0.0083, -0.0798,  0.0189,  0.0598,\n",
       "          -0.0084,  0.1344, -0.0469,  0.0625, -0.0405, -0.0172, -0.0112,  0.1323,\n",
       "           0.0684, -0.0669,  0.0849,  0.0313,  0.0225, -0.1000,  0.0292,  0.0929,\n",
       "           0.0300,  0.0308, -0.0504, -0.0767,  0.0898,  0.0177,  0.0952, -0.0678,\n",
       "           0.0946,  0.0153,  0.0476, -0.0659,  0.0310, -0.0059,  0.0122,  0.1335,\n",
       "          -0.0511, -0.0340,  0.0311, -0.0418,  0.0017, -0.0308, -0.0734, -0.0786,\n",
       "          -0.0032,  0.0695, -0.0548, -0.0131,  0.0365,  0.0314, -0.0058, -0.0322,\n",
       "          -0.0140, -0.0486,  0.0304,  0.0300, -0.0117, -0.1056, -0.0175, -0.0424,\n",
       "           0.0204,  0.0987, -0.0426, -0.0465]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-1.0080e-01,  2.0674e-02,  3.5942e-02,  7.1574e-02, -1.4192e-01,\n",
       "           4.3393e-02,  2.8573e-02, -4.7719e-02, -3.0441e-02,  9.5948e-03,\n",
       "          -6.5527e-03, -2.6619e-02, -8.3168e-02, -1.6255e-01, -1.0522e-01,\n",
       "           2.1503e-02,  4.7099e-03, -2.9455e-02,  1.3446e-01, -1.8144e-02,\n",
       "           4.9070e-02, -1.0257e-01, -4.3000e-02,  3.2196e-02,  2.3618e-02,\n",
       "           5.8472e-02, -7.1924e-02, -2.6877e-02, -4.6799e-02, -6.6839e-02,\n",
       "          -1.4364e-02, -5.6653e-02,  2.7616e-02, -1.1192e-02,  1.7129e-02,\n",
       "          -5.8818e-02,  6.4756e-02,  9.7010e-02,  2.4402e-03, -6.2227e-02,\n",
       "          -4.5461e-02, -1.2685e-02, -4.2894e-02,  2.8817e-02, -8.5053e-02,\n",
       "          -6.7619e-03, -6.7851e-02, -3.4769e-02, -1.5729e-02,  1.1973e-01,\n",
       "           9.9837e-02, -2.6572e-02,  8.8685e-02, -5.7439e-02, -2.6734e-02,\n",
       "           1.2256e-01, -8.5852e-02, -5.3690e-02, -9.9859e-02, -1.7934e-02,\n",
       "          -3.2091e-02, -2.2398e-02,  5.2781e-02, -4.3664e-02,  8.0063e-02,\n",
       "          -1.6829e-02, -4.5531e-02,  4.7714e-02, -2.2549e-02, -7.2477e-03,\n",
       "          -9.7803e-03, -7.9678e-03,  1.7001e-01, -2.6215e-02, -2.2994e-02,\n",
       "          -1.0272e-01,  2.1156e-02,  6.9055e-02,  8.6465e-02,  5.8767e-02,\n",
       "           5.3479e-02,  6.1616e-02,  2.6929e-02,  1.6736e-02, -2.2646e-02,\n",
       "           3.5761e-02, -7.9397e-02,  1.3517e-01,  1.4882e-02,  1.0994e-01,\n",
       "          -1.2226e-01, -3.7671e-02,  8.5188e-02, -3.7818e-03, -2.8396e-02,\n",
       "          -2.4577e-02,  7.1452e-02, -1.9614e-03,  5.6958e-02,  3.2770e-02,\n",
       "          -2.0118e-01, -1.0025e-02, -3.6731e-02, -1.2703e-02, -8.4345e-02,\n",
       "          -4.9886e-02,  3.6858e-02,  4.2257e-02, -6.6859e-02,  7.6823e-03,\n",
       "           8.1948e-02,  1.0263e-02, -1.9058e-02, -4.3817e-03, -8.2564e-02,\n",
       "           2.9137e-02,  4.3350e-04,  1.6961e-02, -5.1009e-02, -1.2067e-01,\n",
       "          -8.0469e-02, -1.7130e-02, -2.1696e-02, -2.2580e-02, -8.0118e-02,\n",
       "          -1.0604e-03, -5.3675e-02,  1.6201e-02, -2.4369e-02,  2.4369e-02,\n",
       "          -6.7061e-03,  2.8924e-03, -4.3638e-02, -3.7492e-02,  2.9169e-02,\n",
       "           1.0198e-01, -3.9954e-03, -5.6517e-02,  8.1611e-02, -8.6308e-03,\n",
       "           1.6180e-02,  4.9015e-02,  7.8002e-02,  8.2478e-03,  4.4013e-02,\n",
       "          -6.3270e-02,  3.8755e-02, -1.9524e-02,  7.1497e-02,  2.0147e-02,\n",
       "          -3.0580e-02,  4.5670e-03,  3.0790e-02,  9.3677e-02,  9.4722e-02,\n",
       "           8.2612e-02, -5.6654e-02, -4.1753e-02,  1.6320e-02,  7.9933e-02,\n",
       "           6.2986e-03,  7.1123e-02,  1.5587e-01,  4.4968e-02, -6.9269e-02,\n",
       "          -1.4218e-02,  1.1616e-01,  1.2501e-02,  2.0304e-02,  9.1513e-02,\n",
       "           1.2715e-02, -1.6750e-02,  1.0394e-01,  3.9885e-02,  3.8614e-03,\n",
       "           4.9016e-02,  2.8366e-02,  2.4871e-02, -5.7460e-02, -8.2568e-03,\n",
       "          -2.9863e-02, -7.1875e-02,  3.2854e-02,  1.2434e-02, -3.7972e-02,\n",
       "          -6.3632e-02,  4.8600e-02,  5.3511e-02, -5.9222e-02, -7.8220e-02,\n",
       "           3.3750e-02,  2.2635e-03,  5.7241e-02,  7.6765e-02,  4.7044e-02,\n",
       "          -8.2485e-02, -7.4205e-02,  4.4019e-02,  1.6715e-02, -2.9570e-02,\n",
       "           6.5862e-02,  5.0989e-02,  1.0380e-02,  1.3263e-01,  2.3675e-02,\n",
       "           1.1380e-01,  6.7803e-02,  1.2436e-02, -5.8679e-02, -4.2585e-02,\n",
       "           1.2613e-01,  1.9451e-02, -3.2884e-03,  2.8967e-02, -4.6445e-02,\n",
       "           2.1693e-02, -4.4695e-02,  1.0430e-02, -9.2743e-03, -7.3397e-03,\n",
       "           1.6781e-01,  5.7718e-02,  1.3009e-02,  3.1284e-02,  6.3541e-02,\n",
       "          -3.0578e-03,  5.4067e-02,  4.6637e-02,  7.5771e-02,  4.9742e-02,\n",
       "          -4.8638e-02, -2.8537e-02, -4.8614e-02,  3.1938e-02,  1.8786e-02,\n",
       "          -1.2960e-01, -3.4012e-03,  1.3161e-01,  7.6770e-03,  1.0572e-01,\n",
       "          -1.8997e-02, -7.5030e-03,  1.7543e-02,  7.9900e-03,  2.4202e-02,\n",
       "          -1.0196e-01,  7.0988e-02,  4.6712e-02, -6.6052e-03, -1.0398e-02,\n",
       "           1.6014e-02, -1.5117e-02, -1.1610e-01,  5.8269e-02, -1.4935e-02,\n",
       "           7.4978e-02, -2.8403e-02, -8.5463e-02, -7.6347e-02,  5.6278e-02,\n",
       "          -5.5147e-03,  9.0170e-02,  8.1039e-02,  1.3460e-02, -2.3504e-02,\n",
       "           1.0798e-01, -1.2436e-01, -5.5696e-02,  2.9676e-03, -2.2383e-02,\n",
       "           3.0948e-02, -6.6091e-02,  9.2169e-02,  5.8049e-03, -4.4927e-02,\n",
       "           1.4567e-01,  1.1758e-01,  6.0201e-02, -7.0334e-02, -1.1027e-02,\n",
       "           2.7120e-02,  4.1077e-02, -4.8429e-02, -1.3000e-02, -2.2853e-03,\n",
       "          -2.3622e-02, -5.2042e-02,  2.0969e-02, -9.6842e-02, -3.9568e-02,\n",
       "           1.1989e-02, -4.3584e-02],\n",
       "         [-9.8098e-02,  2.0254e-02,  3.5224e-02,  7.4806e-02, -1.4225e-01,\n",
       "           4.6882e-02,  3.0479e-02, -4.7696e-02, -3.1613e-02,  9.4829e-03,\n",
       "          -1.0249e-02, -2.7798e-02, -8.0649e-02, -1.6349e-01, -1.0325e-01,\n",
       "           2.2887e-02,  7.2954e-03, -2.4374e-02,  1.3387e-01, -1.8241e-02,\n",
       "           4.9377e-02, -9.9561e-02, -4.4514e-02,  3.0489e-02,  2.3080e-02,\n",
       "           5.9218e-02, -7.5066e-02, -2.8150e-02, -4.5986e-02, -6.3064e-02,\n",
       "          -1.8438e-02, -5.6665e-02,  2.8755e-02, -1.2711e-02,  1.9116e-02,\n",
       "          -5.8829e-02,  6.5515e-02,  9.5608e-02,  5.8526e-03, -6.3755e-02,\n",
       "          -4.4775e-02, -1.2998e-02, -4.1688e-02,  2.8283e-02, -8.4621e-02,\n",
       "          -5.7180e-03, -6.9002e-02, -3.6015e-02, -1.4757e-02,  1.1813e-01,\n",
       "           1.0013e-01, -2.6783e-02,  8.7153e-02, -5.4770e-02, -2.4769e-02,\n",
       "           1.2519e-01, -8.5972e-02, -5.3469e-02, -9.8798e-02, -1.7555e-02,\n",
       "          -3.2698e-02, -1.9852e-02,  5.1315e-02, -4.4683e-02,  7.6739e-02,\n",
       "          -1.7543e-02, -4.7226e-02,  4.6090e-02, -2.0971e-02, -8.0191e-03,\n",
       "          -1.0158e-02, -9.7228e-03,  1.6962e-01, -2.9048e-02, -2.6870e-02,\n",
       "          -1.0372e-01,  2.1227e-02,  6.8576e-02,  8.7749e-02,  5.7560e-02,\n",
       "           5.3044e-02,  5.8412e-02,  2.8345e-02,  1.8848e-02, -2.0869e-02,\n",
       "           3.8364e-02, -8.1736e-02,  1.3040e-01,  1.5141e-02,  1.0921e-01,\n",
       "          -1.2491e-01, -4.0119e-02,  8.6198e-02, -3.4638e-03, -2.7695e-02,\n",
       "          -2.4480e-02,  7.3622e-02, -2.6204e-03,  5.7162e-02,  3.1450e-02,\n",
       "          -2.0074e-01, -6.6062e-03, -3.5507e-02, -1.3151e-02, -8.4939e-02,\n",
       "          -5.1506e-02,  3.7981e-02,  4.3339e-02, -6.5891e-02,  8.5648e-03,\n",
       "           8.3582e-02,  1.0508e-02, -1.9890e-02, -6.1726e-03, -7.5644e-02,\n",
       "           2.8756e-02,  1.9556e-03,  1.5727e-02, -5.2655e-02, -1.2135e-01,\n",
       "          -8.1538e-02, -1.6086e-02, -2.3778e-02, -2.4410e-02, -7.8231e-02,\n",
       "          -2.0500e-03, -5.5400e-02,  1.6323e-02, -1.9999e-02,  2.4759e-02,\n",
       "          -9.9080e-03,  2.2988e-03, -4.4729e-02, -3.5184e-02,  2.8398e-02,\n",
       "           1.0176e-01, -4.4894e-03, -5.3568e-02,  8.4784e-02, -9.8647e-03,\n",
       "           1.4753e-02,  5.0719e-02,  7.8083e-02,  1.0631e-02,  4.3613e-02,\n",
       "          -6.6399e-02,  3.8502e-02, -1.7216e-02,  7.1858e-02,  2.2282e-02,\n",
       "          -2.9166e-02,  6.7700e-03,  3.0037e-02,  9.6164e-02,  9.2911e-02,\n",
       "           8.4204e-02, -5.7744e-02, -3.6535e-02,  1.7922e-02,  8.2243e-02,\n",
       "           6.0700e-03,  7.4295e-02,  1.5121e-01,  4.5920e-02, -6.5602e-02,\n",
       "          -1.6671e-02,  1.1557e-01,  1.0143e-02,  2.1954e-02,  9.2978e-02,\n",
       "           1.1192e-02, -1.5415e-02,  1.0656e-01,  3.8521e-02, -2.0384e-03,\n",
       "           5.2812e-02,  2.5710e-02,  2.4965e-02, -5.7292e-02, -8.5742e-03,\n",
       "          -2.8305e-02, -7.2651e-02,  3.4816e-02,  1.3216e-02, -4.0744e-02,\n",
       "          -6.2821e-02,  5.0525e-02,  5.1531e-02, -6.0349e-02, -7.7726e-02,\n",
       "           3.4375e-02,  1.0598e-04,  5.8123e-02,  7.7805e-02,  4.5314e-02,\n",
       "          -8.2109e-02, -7.4593e-02,  4.3325e-02,  1.5694e-02, -3.0285e-02,\n",
       "           6.8463e-02,  4.9446e-02,  7.0269e-03,  1.3224e-01,  2.4129e-02,\n",
       "           1.1435e-01,  6.4986e-02,  1.4376e-02, -6.1556e-02, -4.2747e-02,\n",
       "           1.3057e-01,  2.2294e-02, -2.7015e-03,  2.8536e-02, -4.5364e-02,\n",
       "           2.1114e-02, -4.0536e-02,  1.2363e-02, -1.1524e-02, -1.0980e-02,\n",
       "           1.6613e-01,  5.4609e-02,  1.4974e-02,  3.1129e-02,  6.3075e-02,\n",
       "          -4.8019e-03,  5.6405e-02,  4.6683e-02,  7.5191e-02,  5.2375e-02,\n",
       "          -5.1436e-02, -2.8609e-02, -4.8116e-02,  3.3321e-02,  1.7504e-02,\n",
       "          -1.3082e-01, -5.8490e-03,  1.2788e-01,  8.2728e-03,  1.0542e-01,\n",
       "          -1.9068e-02, -7.4560e-03,  2.1857e-02,  7.2495e-03,  2.6432e-02,\n",
       "          -1.0280e-01,  6.9973e-02,  4.8470e-02, -8.7817e-03, -8.3960e-03,\n",
       "           1.6940e-02, -1.3962e-02, -1.1446e-01,  5.7499e-02, -1.5664e-02,\n",
       "           7.4304e-02, -2.5146e-02, -8.3789e-02, -7.6882e-02,  5.8033e-02,\n",
       "          -3.1858e-03,  9.0376e-02,  8.3945e-02,  1.0901e-02, -2.1609e-02,\n",
       "           1.0587e-01, -1.2705e-01, -5.6490e-02,  3.5216e-03, -2.3209e-02,\n",
       "           3.0290e-02, -6.4390e-02,  9.2160e-02,  4.4014e-03, -4.3511e-02,\n",
       "           1.4345e-01,  1.1650e-01,  5.8123e-02, -7.2216e-02, -6.4437e-03,\n",
       "           2.6455e-02,  4.1698e-02, -4.8361e-02, -1.1010e-02, -1.9453e-03,\n",
       "          -2.2691e-02, -5.5137e-02,  2.3918e-02, -9.6548e-02, -3.7901e-02,\n",
       "           1.1970e-02, -4.5556e-02]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the model() on data runs the forward pass through the network\n",
    "VAE()(data_train_tensor[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_loss(x_decoded_mean, x, z_mean, z_logvar):\n",
    "    reconstruction_loss = F.binary_cross_entropy(x_decoded_mean, x, size_average=False)\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_tensor_loader = torch.utils.data.TensorDataset(torch.from_numpy(data_train))\n",
    "train_loader = torch.utils.data.DataLoader(data_train_tensor_loader, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 30\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(MODEL):\n",
    "    # Implement Tomorrow\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
