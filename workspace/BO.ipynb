{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['MKL_NUM_THREADS'] = '1'\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For automatically reloading import modules... allows you to run changes to code in jupyter without having to reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import h5py\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "\n",
    "from photocatalysis.learners_treesearch import get_population_completed\n",
    "from photocatalysis.deeplearning.helpers import get_charset, smiles_to_onehot, one_hot_to_smile, plot_model_performance, verify_smile\n",
    "\n",
    "import selfies as sf\n",
    "from photocatalysis.deeplearning.helpers import get_charset_selfies, selfies_to_onehot\n",
    "\n",
    "from photocatalysis.deeplearning.models import VAE, train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f'CUDA GPU Available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/home/btpq/bt308495/Thesis/frames/DF_COMPLETE.json', orient='split')\n",
    "df = get_population_completed(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All molecules encode and decode successfully\n",
    "molecule_selfies = [sf.encoder(s) for s in df.molecule_smiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 60\n",
    "sos_token = \"X\"\n",
    "eos_token = \"[nop]\"\n",
    "\n",
    "char_list, max_selfie_len = get_charset_selfies(molecule_selfies, sos_token=sos_token, eos_token=eos_token)\n",
    "X = selfies_to_onehot(molecule_selfies, char_list, max_selfie_len=input_size)\n",
    "y = df.utility_function.values.astype(np.float32)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 80\n",
    "sos_token = 'X'\n",
    "eos_token = ' '\n",
    "\n",
    "char_list, max_smi_len = get_charset(df.molecule_smiles, sos_token=sos_token, eos_token=eos_token)\n",
    "X = smiles_to_onehot(df.molecule_smiles, char_list, input_size)\n",
    "y = df.utility_function.values.astype(np.float32)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/5/15 Train/Val/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Torch Tensor\n",
    "X_tensor = torch.from_numpy(X)\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "\n",
    "X_valid_tensor = torch.from_numpy(X_valid)\n",
    "y_valid_tensor = torch.from_numpy(y_valid)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# Create Dataset/Dataloader for model training\n",
    "data_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "data_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "data_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=250, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid, batch_size=250, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=250, shuffle=True)\n",
    "\n",
    "# D = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_tensor[:250], y_train_tensor[:250]), batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goindex = np.where(char_list == 'X')[0][0]\n",
    "gotoken = torch.FloatTensor(len(char_list)).zero_()\n",
    "gotoken[goindex] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_params(params_file, model_training_file, model_weights_file, device='cpu', assert_prob_sample=True):\n",
    "    # Load Model definition parameters\n",
    "    with open(params_file, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "    # Load Training Losses\n",
    "    with open(model_training_file, 'rb') as f:\n",
    "        tls, vls = pickle.load(f)\n",
    "\n",
    "    if assert_prob_sample: params['probabilistic_sampling'] = True\n",
    "    \n",
    "    MODEL = VAE(**params).to(device)\n",
    "    MODEL.load_state_dict(torch.load(model_weights_file, map_location=torch.device(device)))\n",
    "    MODEL.eval()\n",
    "\n",
    "    return MODEL, tls, vls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_latent(X_tensor, MODEL):\n",
    "    with torch.no_grad():\n",
    "        X = deepcopy(X_tensor)\n",
    "        X = X.to(device)\n",
    "\n",
    "        mu_z, logvar_z = MODEL.encode(X)\n",
    "        z = MODEL.reparameterize(mu_z, logvar_z)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "params_file = '/home/btpq/bt308495/Thesis/photocatalysis/workspace/train_teacher_prop/VAE_model_params.pckl'\n",
    "training_file = '/home/btpq/bt308495/Thesis/photocatalysis/workspace/train_teacher_prop/VAE_losses_epoch60.pckl'\n",
    "weights_file = '/home/btpq/bt308495/Thesis/photocatalysis/workspace/train_teacher_prop/VAE_model_params_epoch60.pt'\n",
    "\n",
    "model, tls, vls = load_model_from_params(params_file, training_file, weights_file, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls, vls = plot_model_performance(tls, vls, name='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import gaussian_process as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_tensor = encode_latent(X_train_tensor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a maximally diverse set of initial molecules\n",
    "# For now, just sample randomly\n",
    "ri = np.random.randint(z_train_tensor.shape[0], size=2000)\n",
    "Z, Y = z_train_tensor[ri].numpy(), y_train[ri]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5\n",
    "n_restarts_optimizer = 10\n",
    "normalize_y = True\n",
    "random_state = 42\n",
    "\n",
    "gpr_prop = gp.GaussianProcessRegressor(alpha=alpha,\n",
    "                                       n_restarts_optimizer=n_restarts_optimizer,\n",
    "                                       normalize_y=normalize_y,\n",
    "                                       random_state=random_state).fit(Z, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_prop.score(z_train_tensor.numpy(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gpr_prop.predict(z_train_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
    "plt.plot(y_train, y_train, 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian Optimization, we'd like to use a surrogate model (GPR for example) to approximate the utility of unknown regions in the latent space Z, then subsequently optimize an aqcuisition function over the surrogate model to sample regions of this space most likely to contain high performing molecules (i.e. molecules that maximize utility). A candidate molecule is chosen that is expected to have high utility (according to the model), the molecule is evaluated in some black-box fashion (an expensive simulation of the molecules IP/dGmax), and the surrogate model is subsequently retrained on the points it had surveyed before as well as the newly determined point. Re-training 'updates the prior' of the model to incorporate new simulation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, instead of optimizing the black box function property=F(molecule), we'll use another surrogate model as a placeholder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from sklearn import gaussian_process as gp\n",
    "from photocatalysis.evaluate import evaluate_substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(zz):\n",
    "    # Black-box objective function to maximize\n",
    "    # Normally, would be Utility = F(latent_code)... to do later\n",
    "    return gpr_prop.predict(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(X, Y_sampled, surrogate_model, xi=0.01):\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        X: Points at which EI shall be computed (m x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        surrogate_model: A GaussianProcessRegressor fitted to samples.\n",
    "        xi: Exploitation-exploration trade-off parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "    Y, sigma = surrogate_model.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    best_sampled_Y = np.max(Y_sampled)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        # EI equation.... where Y (predictions) represents the mean of the surrogate gpr model, and sigma represents the std/uncertainty estimate of the prediction\n",
    "        imp = Y - best_sampled_Y - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_location(acquisition_function, Y_sampled, surrogate_model, bounds, n_restarts=25):\n",
    "    '''\n",
    "    Proposes the next sampling point by optimizing the acquisition function over the surrogate model.\n",
    "    In practice, optimizing this function is much cheaper than directly optimizing an expensive black-box function.\n",
    "    \n",
    "    Args:\n",
    "        acquisition: Acquisition function.\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        surrogate_model: A GaussianProcessRegressor fitted to samples.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "    dim = surrogate_model.n_features_in_\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition_function(X.reshape(-1, dim), Y_sampled, surrogate_model)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    print(f'Optimizing Acquisition Function: {acquisition_function.__name__} w/ {n_restarts} restarts')\n",
    "\n",
    "    mins_x, mins_val = [], []\n",
    "    start_locations = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim))\n",
    "    for x0 in tqdm(start_locations):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B') \n",
    "\n",
    "        # Log each opt result to have multiple latent vectors to choose from,\n",
    "        # incase the min_val latent vector decodes to garbage\n",
    "        min_val = res.fun\n",
    "        min_x = res.x.reshape(1,-1)\n",
    "\n",
    "        mins_x.append(min_x)\n",
    "        mins_val.append(min_val)\n",
    "\n",
    "        # if res.fun < min_val:\n",
    "        #     # If choosing the best optimization\n",
    "        #     min_val = res.fun\n",
    "        #     min_x = res.x\n",
    "\n",
    "    print(f'Best Expected Improvement in Utility: {min(mins_val)}')           \n",
    "            \n",
    "    return np.array(mins_x), np.array(mins_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_logits(Logits, character_list, num_decode_attempts=100):\n",
    "    # Probabilistic multinomial sampling of softmax output logist\n",
    "    # and subsequent bare smile string decoding w/o validation\n",
    "    assert Logits.shape[1] == character_list.size, 'Logit length doesnt match character_set length'\n",
    "    decoded_smiles = []\n",
    "\n",
    "    for _ in range(num_decode_attempts):\n",
    "        indx = torch.multinomial(Logits, 1)\n",
    "        one_hot = torch.zeros_like(Logits)\n",
    "        one_hot.scatter_(1, indx, 1)\n",
    "        \n",
    "        smile = one_hot_to_smile(one_hot, character_list)\n",
    "        decoded_smiles.append(smile)\n",
    "    \n",
    "    return decoded_smiles\n",
    "\n",
    "def find_valid_smiles_from_logits(logits_batch, character_list, num_decode_attempts=100):\n",
    "    # Look through logits batch, decode probabilistically, and return a list of valid smile strings\n",
    "\n",
    "    valid_smiles = []\n",
    "    for l in logits_batch:\n",
    "        smiles = decode_logits(l, character_list, num_decode_attempts=num_decode_attempts)\n",
    "\n",
    "        for s in smiles:\n",
    "            if verify_smile(s):\n",
    "                valid_smiles.append(s)\n",
    "\n",
    "    return valid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(smile, CALC_PARAMS, scratch_dir=None):\n",
    "    base = os.getcwd()\n",
    "\n",
    "    if scratch_dir is not None:\n",
    "        os.mkdir(scratch_dir)\n",
    "\n",
    "    try:\n",
    "        # ip, rdg, asites, rds, essi = output\n",
    "        output = evaluate_substrate(smile, CALC_PARAMS, scratch_dir=scratch_dir) # scratch_dir=SCRATCH_DIR)\n",
    "        print('Success!')\n",
    "        return output\n",
    "    \n",
    "    except Exception as e:\n",
    "        output = None\n",
    "\n",
    "        print('Error Encountered!')\n",
    "        print(e)\n",
    "        os.system('echo \"{}\" >> errors.txt'.format(e))\n",
    "        os.chdir(base)\n",
    "        return None\n",
    "    \n",
    "def evaluate_latent(z, CALC_PARAMS, num_run_throughs=10, num_decode_attempts=100):\n",
    "    # Given a latent vector, return its valid smile and associated utility\n",
    "    \n",
    "    # The VAE is probabilistic in decoding (i.e. a softmax is sampled to determine which character is fed next into the RNN-decoder)\n",
    "    # and probabilistic in sampling the output (i.e. the output logits define a probability distb. over the possible characters,\n",
    "    # and multiple smiles can be sampled)\n",
    "\n",
    "    # So we can sample multiple model calls (num_run_throughs) and multiple smiles from the resulting logits (num_decode_attempts)\n",
    "\n",
    "    z = torch.from_numpy(z.astype(np.float32))\n",
    "    z_repeat = z.repeat(num_run_throughs, 1)\n",
    "\n",
    "    logits_batch, _ = model.decode(z_repeat)\n",
    "\n",
    "    print('Decoding latent representation into valid smiles')\n",
    "    smiles = find_valid_smiles_from_logits(logits_batch, char_list, num_decode_attempts=num_decode_attempts)\n",
    "\n",
    "    # print('Premature Breaking before simulation/evaluation')\n",
    "    # return smiles\n",
    "\n",
    "    if smiles:\n",
    "        print('Success!')\n",
    "        # atleast one valid smile exists\n",
    "        # evaluate until first successful evaluation occurs and return results\n",
    "        for j, smile in enumerate(smiles):\n",
    "            print(f'Evaluating candidate {j}')\n",
    "            print('--------------------')\n",
    "\n",
    "            smile = smile.strip(' ') # remove any trailing whitespaces\n",
    "            output = run_evaluation(smile, CALC_PARAMS, scratch_dir=f'eval{j}')\n",
    "\n",
    "            if output is not None:\n",
    "                return (smile, output)\n",
    "        \n",
    "        # No Succesful run (highly unlikely we get here, but just in case)\n",
    "        print('No Successful evaluation :(')\n",
    "        return None\n",
    "    else:\n",
    "        # No valid smiles\n",
    "        print('Fail!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bo_iter = 10 # number of function evaluations used to update GPR surrogate prior\n",
    "n_init = 2000 # number of initial representations to train the surrogate model\n",
    "\n",
    "num_run_throughs = 50\n",
    "num_decode_attempts = 500\n",
    "calc_params = {'gfn':2, 'acc':0.2, 'etemp':298.15, 'gbsa':'water'}\n",
    "\n",
    "run_dir = '/home/btpq/bt308495/Thesis/run/'\n",
    "os.chdir(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode training set\n",
    "z_train_tensor = encode_latent(X_train_tensor, model)\n",
    "\n",
    "# Select initial data points to start optimization\n",
    "ri = np.random.randint(z_train_tensor.shape[0], size=n_init)\n",
    "Z_sampled, Y_sampled = z_train_tensor[ri].numpy(), y_train[ri]\n",
    "\n",
    "# Sequence of min-max pairs for each latent dim\n",
    "bounds = np.vstack([z_train_tensor.numpy().min(axis=0), z_train_tensor.numpy().max(axis=0)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = gp.GaussianProcessRegressor(alpha=1e-5,\n",
    "                                  n_restarts_optimizer=10,\n",
    "                                  normalize_y=True,\n",
    "                                  random_state=42)\n",
    "\n",
    "fresh_evaluated_smiles = [] # saving newly created smiles and their associated properties\n",
    "\n",
    "for i in range(n_bo_iter):\n",
    "    print('###############################')\n",
    "    print(f'Bayesian Optimization iter {i}')\n",
    "    \n",
    "    # Update Gaussian process with existing samples\n",
    "    print('\\nFitting Surrogate Model')\n",
    "    gpr.fit(Z_sampled, Y_sampled)\n",
    "\n",
    "    # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "    Z_candidates, U_candidates = propose_location(expected_improvement, Y_sampled, gpr, bounds, n_restarts=25)\n",
    "\n",
    "    # Decode and evaluate smiles for their utility\n",
    "    Z_candidates_sorted = Z_candidates[np.argsort(U_candidates)]\n",
    "    for j, Z_candidate in enumerate(Z_candidates_sorted):\n",
    "        # Try decoding the best candidate\n",
    "        output = evaluate_latent(Z_candidate, calc_params, num_run_throughs=num_run_throughs, num_decode_attempts=num_decode_attempts)\n",
    "        \n",
    "        if output is not None:\n",
    "            s, o = output\n",
    "            ip, rdg, _, _, _ = o\n",
    "            Y_candidate = ip - rdg\n",
    "            fresh_evaluated_smiles.append((s, ip, rdg))\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Try next best candidate\n",
    "            print('Attempting to decode/evaluate next best candidate')\n",
    "            continue\n",
    "    \n",
    "    if len(fresh_evaluated_smiles)-1 != i:\n",
    "        raise ValueError(f'Non Successful smiles evaluated in BO iter {i}')\n",
    "    \n",
    "    # Add sample to previous samples\n",
    "    Z_sampled = np.vstack((Z_sampled, Z_candidate))\n",
    "    Y_sampled = np.vstack((Y_sampled, Y_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
