{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For automatically reloading import modules... allows you to run changes to code in jupyter without having to reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import h5py\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "\n",
    "from photocatalysis.learners_treesearch import get_population_completed\n",
    "from photocatalysis.deeplearning.helpers import get_charset, smiles_to_onehot, one_hot_to_smile\n",
    "\n",
    "from photocatalysis.deeplearning.models import VAE, train_epoch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "\n",
    "from torch import nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f'CUDA GPU Available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/home/btpq/bt308495/Thesis/frames/DF_COMPLETE.json', orient='split')\n",
    "df = get_population_completed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 80\n",
    "sos_token = 'X'\n",
    "\n",
    "char_list, max_smi_len = get_charset(df.molecule_smiles, sos_token=sos_token)\n",
    "data = smiles_to_onehot(df.molecule_smiles, char_list, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/5/15 Train/Val/Test Split\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "data_valid, data_test = train_test_split(data_test, test_size=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_tensor = torch.from_numpy(data_train)\n",
    "data_valid_tensor = torch.from_numpy(data_valid)\n",
    "data_test_tensor = torch.from_numpy(data_test)\n",
    "\n",
    "# data_train_tensor_loader = torch.utils.data.TensorDataset(data_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(data_train_tensor, batch_size=250, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid_tensor, batch_size=250, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(data_test_tensor, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher forcing testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "goindex = np.where(char_list == 'X')[0][0]\n",
    "gotoken = torch.FloatTensor(len(char_list)).zero_()\n",
    "gotoken[goindex] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (conv_1): Conv1d(80, 9, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(9, 9, kernel_size=(5,), stride=(1,))\n",
       "  (conv_3): Conv1d(9, 10, kernel_size=(7,), stride=(1,))\n",
       "  (linear_0): Linear(in_features=80, out_features=435, bias=True)\n",
       "  (mean_linear_1): Linear(in_features=435, out_features=292, bias=True)\n",
       "  (var_linear_2): Linear(in_features=435, out_features=292, bias=True)\n",
       "  (linear_3): Linear(in_features=292, out_features=292, bias=True)\n",
       "  (stacked_gru): GRU(292, 501, num_layers=3, batch_first=True)\n",
       "  (terminalGRU): teacherGRU(\n",
       "    (cell): advGRUCell(523, 501)\n",
       "    (linear): Linear(in_features=501, out_features=22, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gotoken.to(device)\n",
    "\n",
    "# NEW MODEL\n",
    "model = VAE(INPUT_SIZE=input_size,\n",
    "            CHARSET_LEN=len(char_list),\n",
    "            LATENT_DIM=292,\n",
    "            filter_sizes=(9,9,10),\n",
    "            kernel_sizes=(5,5,7),\n",
    "            eps_std=1e-2,\n",
    "            useTeacher=True,\n",
    "            gotoken=gotoken,\n",
    "            probabilistic_sampling=True).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Forcing Devolopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# NEW MODEL\n",
    "model = VAE(INPUT_SIZE=input_size,\n",
    "            CHARSET_LEN=len(char_list),\n",
    "            LATENT_DIM=292,\n",
    "            filter_sizes=(9,9,10),\n",
    "            kernel_sizes=(5,5,7),\n",
    "            eps_std=1e-2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train_tensor[:2]\n",
    "z = model.reparameterize(*model.encode(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass latent vector through fully connected layer (output shape == input shape)\n",
    "zout = F.selu(model.linear_3(z))\n",
    "\n",
    "# Repeat latent vectors seq_len number of times (batchsize, seq_len, latent_dim)\n",
    "zprime = zout.view(zout.size(0), 1, zout.size(-1)).repeat(1, model.INPUT_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ar, har = model.stacked_gru(zprime)\n",
    "out_ar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character by character feeding into the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zprime_char = zout.view(zout.size(0), 1, zout.size(-1))\n",
    "zprime_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0, hs0 = model.stacked_gru(zprime_char)\n",
    "out0.shape, hs0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0[:, 0, :] == out_ar[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, hs1 = model.stacked_gru(zprime_char, hs0)\n",
    "out1.shape, hs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1[:, 0, :] == out_ar[:, 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = []\n",
    "for i in range(model.INPUT_SIZE):\n",
    "    if i == 0:\n",
    "        out, hs = model.stacked_gru(zprime_char)\n",
    "        decoder_outputs.append(out)\n",
    "    else:\n",
    "        out, hs = model.stacked_gru(zprime_char, hs)\n",
    "        decoder_outputs.append(out)\n",
    "\n",
    "out = torch.cat(decoder_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(out_ar == out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21+292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([z, X[:, 0, :]], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = model.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datareading and GO token for teacher forcing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_double(smi: str) -> str:\n",
    "    for s, w in zip(['Br', 'Cl', 'Si'], [\"Ö\", \"Ä\", \"Å\"]):\n",
    "        smi = smi.replace(s, w)\n",
    "    return smi\n",
    "\n",
    "class OnehotEncoder(object):\n",
    "    def __init__(self, alphabet, sos_token=\"X\", maxlen=120):\n",
    "        # Start-of-seq token is 'X' by default\n",
    "        alphabet = ''.join([\" \", sos_token] + alphabet)\n",
    "        alphabet = replace_double(alphabet)\n",
    "        self.alphabet = {k: v for v, k in enumerate(alphabet)}\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.alphabetlen = len(alphabet)\n",
    "\n",
    "    def __call__(self, smi: str):\n",
    "        indices = torch.LongTensor(self.maxlen, 1)\n",
    "        indices.zero_()\n",
    "        smi = replace_double(smi.rstrip())\n",
    "        for i, char in enumerate(smi):\n",
    "            indices[i] = self.alphabet[char]\n",
    "        one_hot = torch.zeros(self.maxlen, self.alphabetlen)\n",
    "        one_hot.scatter_(1, indices, 1)\n",
    "        return one_hot\n",
    "    \n",
    "class OnehotDecoder(object):\n",
    "\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = [\" \", \"\"] + alphabet  # Replace GO token with empty string\n",
    "\n",
    "    def decode(self, onehot: torch.FloatTensor):\n",
    "        if onehot.dim() == 2:\n",
    "            onehot = onehot[None, :, :]\n",
    "        maxs, indices = torch.max(onehot, 2)\n",
    "        smiles = []\n",
    "        for i in range(indices.size()[0]):\n",
    "            chars = [self.alphabet[index] for index in indices[i,].view(-1)]\n",
    "            smiles.append(\"\".join(chars).strip())\n",
    "        return smiles\n",
    "\n",
    "    def decode_int(self, inds: torch.LongTensor):\n",
    "        if inds.dim() == 2:\n",
    "            inds = inds[None, :, :]\n",
    "        smiles = []\n",
    "        for i in range(inds.size()[0]):\n",
    "            chars = [self.alphabet[index] for index in inds[i,].view(-1)]\n",
    "            smiles.append(\"\".join(chars).strip())\n",
    "            return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESReader(data.Dataset):\n",
    "\n",
    "    def __init__(self, smiles_list, alphabet, subset=(0, None), maxlen=120):\n",
    "        self.onehotencoder = OnehotEncoder(alphabet, maxlen=maxlen)\n",
    "        self.smiles = smiles_list[subset[0]:subset[1]]\n",
    "        self.alphabet = self.onehotencoder.alphabet\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        smi = self.smiles[index]\n",
    "        one_hot = self.onehotencoder(smi)\n",
    "        cat_hot = torch.LongTensor(1).zero_()\n",
    "        cat_hot[0] = -1\n",
    "        return one_hot, cat_hot\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(char_list[:-1])\n",
    "smi_reader = SMILESReader(df.molecule_smiles.tolist(), alphabet, maxlen=80)\n",
    "smi_decoder = OnehotDecoder(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_decoder.decode(smi_reader[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_reader[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_reader.alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goindex = 1\n",
    "batch_size = 256\n",
    "gotoken = torch.FloatTensor(len(smi_reader.alphabet)).zero_()\n",
    "gotoken[goindex] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gotoken.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_z = z.view(z.size(0), 1, z.size(-1)).repeat(1, model.INPUT_SIZE, 1)\n",
    "o, h = model.stacked_gru(repeat_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_independent = o.contiguous().view(-1, o.size(-1))\n",
    "logits = model.linear_4(out_independent)\n",
    "\n",
    "y_test = F.softmax(logits, dim=1)\n",
    "y = y_test.contiguous().view(o.size(0), -1, y_test.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "F.softmax(model.linear_4(o), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = nn.Linear(501, 21)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.softmax(no, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape, out_independent.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher/Terminal GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(char_list[:-1]) #alphabet not including empty space char ' '... to be added in SMILESReader\n",
    "\n",
    "smiles = df.molecule_smiles.tolist() # bare smiles list\n",
    "smiles_go = df.molecule_smiles.map(lambda x: 'X'+x).tolist() # all smiles prepended with GO/SOS token\n",
    "\n",
    "smi_reader = SMILESReader(smiles, alphabet, maxlen=80)\n",
    "smi_reader_go = SMILESReader(smiles_go, alphabet, maxlen=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHARS = len(smi_reader.alphabet)\n",
    "goindex = smi_reader.alphabet['X']\n",
    "gotoken = torch.FloatTensor(N_CHARS).zero_()\n",
    "gotoken[goindex] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([smi_reader[i][0] for i in range(len(smi_reader))])\n",
    "XGO = torch.stack([smi_reader_go[i][0] for i in range(len(smi_reader))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(input):\n",
    "    noise = torch.rand(input.size())\n",
    "    if input.is_cuda:\n",
    "        noise = noise.cuda()\n",
    "    eps = 1e-9\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    return Variable(noise)\n",
    "\n",
    "def gumbel_softmax_sample(input_, hard=True, temperature=1., uselogprop=True):\n",
    "    # Softmax has some undesired behaviour in pytorch 0.1.12:\n",
    "    # if the input is 2D then it operates over dimension 1, if the input is 3D it operates on dimension 0\n",
    "    # thus, to sample a 3D tensor (timestep, batch, chars) correctly, we have to reshape it to (timestep * batch, chars)\n",
    "    # before we can continue\n",
    "    size = tuple(input_.size())\n",
    "    if input_.dim() >= 3:\n",
    "        input_ = input_.view(-1, input_.size(-1))\n",
    "    noise = sample_gumbel(input_)\n",
    "    if uselogprop:\n",
    "        a = (input_ + noise) / temperature\n",
    "        a = F.log_softmax(a)\n",
    "    else:\n",
    "        a = (torch.log(input_) + noise) / temperature\n",
    "        a = F.softmax(a)\n",
    "\n",
    "    if hard == True:\n",
    "        _, max_inx = torch.max(a, a.dim() - 1)\n",
    "        if a.is_cuda:\n",
    "            a_hard = torch.cuda.FloatTensor(a.size()).zero_().scatter_(a.dim() - 1, max_inx.data, 1.0)\n",
    "        else:\n",
    "            print(a.shape)\n",
    "            print((a.dim() - 1, max_inx.data, 1.0))\n",
    "            a_hard = torch.FloatTensor(a.size()).zero_().scatter_(a.dim() - 1, max_inx.data, 1.0)\n",
    "        a2 = a.clone()\n",
    "        tmp = Variable(a_hard - a2.data)\n",
    "        tmp.detach_()\n",
    "\n",
    "        a = tmp + a\n",
    "\n",
    "    return a.view(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class advGRUCell(nn.GRUCell):\n",
    "    def __init__(self, input_size, hidden_size, activation=F.tanh, inner_activation=F.sigmoid):\n",
    "        super(advGRUCell, self).__init__(input_size, hidden_size, bias=True)\n",
    "        self.activation = activation\n",
    "        self.inner_activation = inner_activation\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        if hx is None:\n",
    "            hx = torch.autograd.Variable(input.data.new(\n",
    "                input.size(0),\n",
    "                self.hidden_size).zero_(), requires_grad=False)\n",
    "        gi = F.linear(input, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        resetgate = self.inner_activation(i_r + h_r)\n",
    "        inputgate = self.inner_activation(i_z + h_z)\n",
    "        preactivation = i_n + resetgate * h_n\n",
    "        newgate = self.activation(preactivation)\n",
    "        hy = newgate + inputgate * (hx - newgate)\n",
    "\n",
    "        return hy, preactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oldteacherGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=F.log_softmax,\n",
    "                 gru_activation=F.tanh, gru_inner_activation=F.sigmoid, useTeacher=True,\n",
    "                 gotoken=None, multinomial=True):\n",
    "        if useTeacher and gotoken is None:\n",
    "            raise ValueError(\"Need to provide a gotoken when using teachers forcing\")\n",
    "        super(oldteacherGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.teacher = useTeacher\n",
    "        self.gotoken = gotoken\n",
    "        self.multinomial = multinomial\n",
    "        \n",
    "        if useTeacher:\n",
    "            self.cell = advGRUCell(input_size=input_size + output_size, hidden_size=hidden_size,\n",
    "                                   activation=gru_activation, inner_activation=gru_inner_activation)\n",
    "        else:\n",
    "            self.cell = advGRUCell(input_size=input_size, hidden_size=hidden_size,\n",
    "                                   activation=gru_activation, inner_activation=gru_inner_activation)\n",
    "            \n",
    "        if self.multinomial:\n",
    "            self.sample = torch.multinomial\n",
    "        else:\n",
    "            def topi(matrix, top):\n",
    "                return torch.topk(matrix, top)[1]\n",
    "            self.sample = topi\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, y, groundTruth=None, hx=None, max_length=None, temperature=0.5):\n",
    "        # if self.teacher and self.training and groundTruth is None:\n",
    "        #    raise NotImplementedError(\"No groundTruth in teachers trainingsmode\")\n",
    "        batch_size = y.size(0)\n",
    "        seq_length = y.size(1)\n",
    "        if max_length is None:\n",
    "            max_length = seq_length\n",
    "\n",
    "        output = []\n",
    "        sampled_output = []\n",
    "        preactivation = []\n",
    "        if hx is None:\n",
    "            # Initialize hidden-state as zeros\n",
    "            # hx = Variable(x.data.new(batch_size, self.hidden_size).zero_(), requires_grad=False)\n",
    "            hx = y.data.new(batch_size, self.hidden_size).zero_()\n",
    "            \n",
    "        for i in range(max_length):\n",
    "            if self.teacher and i == 0:\n",
    "                # gotoken_target = Variable(self.gotoken.repeat(batch_size, 1), requires_grad=False)\n",
    "                # input_ = torch.cat([x[:, i, :], gotoken_target.type_as(x)], dim=-1)\n",
    "                gotoken_target = self.gotoken.repeat(batch_size, 1)\n",
    "                input_ = torch.cat([y[:, i, :], gotoken_target], dim=-1)\n",
    "                hx, pre = self.cell(input_, hx=hx)\n",
    "\n",
    "            elif self.teacher and groundTruth is not None:\n",
    "                target = groundTruth[:, i - 1, :]\n",
    "                input_ = torch.cat([y[:, i, :], target], dim=-1)\n",
    "                hx, pre = self.cell(input_, hx=hx)\n",
    "\n",
    "            elif self.teacher and groundTruth is None:\n",
    "                target = sampled_output[-1]\n",
    "                input_ = torch.cat([y[:, i, :], target], dim=-1)\n",
    "                hx, pre = self.cell(input_, hx=hx)\n",
    "                \n",
    "            elif not self.teacher:\n",
    "                input_ = y[:, i, :]\n",
    "                hx, pre = self.cell(input_, hx=hx)\n",
    "            else:\n",
    "                raise NotImplementedError(\"TeacherGRU. Unknown operation mode\")\n",
    "            \n",
    "            output_ = self.activation(self.linear(hx)) #project into charset space with log_softmax activation\n",
    "            output.append(output_.view(batch_size, 1, self.output_size))\n",
    "            preactivation.append(pre.view(batch_size, 1, self.hidden_size))\n",
    "\n",
    "            ### Gumbel sampling\n",
    "            # sampled_output.append(gumbel_softmax_sample(output_, hard=True, temperature=temperature, uselogprop=True))\n",
    "\n",
    "            indices = self.sample(torch.exp(output_), 1)\n",
    "            one_hot = output_.data.new(output_.size(0), self.output_size).zero_() # originally was self.hidden_size, although i think this is a mistake\n",
    "            one_hot.scatter_(1, indices, 1)\n",
    "            # one_hot = Variable(one_hot)\n",
    "            sampled_output.append(one_hot)\n",
    "\n",
    "        output = torch.cat(output, 1) # log probabilites\n",
    "        preactivation = torch.cat(preactivation, 1)\n",
    "        sampled_output = torch.stack(sampled_output, 1)\n",
    "\n",
    "        return output, preactivation, sampled_output, hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class teacherGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 gru_activation=F.tanh, gru_inner_activation=F.sigmoid,\n",
    "                 gotoken=None, state_dict=None, probabilistic_sampling=True):\n",
    "        \n",
    "        if gotoken is None:\n",
    "            raise ValueError(\"Need to provide a gotoken when using teachers forcing\")\n",
    "        \n",
    "        super(teacherGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.gotoken = gotoken\n",
    "        \n",
    "        self.cell = advGRUCell(input_size=input_size + output_size, hidden_size=hidden_size, \n",
    "                               activation=gru_activation, inner_activation=gru_inner_activation)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        if state_dict is not None:\n",
    "            self.cell.load_state_dict(state_dict[0])\n",
    "            self.linear.load_state_dict(state_dict[1])\n",
    "\n",
    "        if probabilistic_sampling:\n",
    "            self.sample = torch.multinomial\n",
    "        else:\n",
    "            def topi(matrix, top):\n",
    "                return torch.topk(matrix, top)[1]\n",
    "            self.sample = topi\n",
    "\n",
    "    def forward(self, y, groundTruth=None, hx=None):\n",
    "        batch_size = y.size(0)\n",
    "        seq_length = y.size(1)\n",
    "\n",
    "        output = []\n",
    "        sampled_output = []\n",
    "        preactivation = []\n",
    "\n",
    "        target = self.gotoken.repeat(batch_size, 1)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = y.data.new(batch_size, self.hidden_size).zero_()\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            input_ = torch.cat([y[:, i, :], target], dim=-1)\n",
    "            hx, pre = self.cell(input_, hx=hx)\n",
    "            output_ = F.log_softmax(self.linear(hx), dim=1)\n",
    "            \n",
    "            # Sampling\n",
    "            probs = torch.exp(output_)\n",
    "            indices = self.sample(probs, 1)\n",
    "            one_hot = output_.data.new(output_.size(0), self.output_size).zero_() # originally was self.hidden_size, although i think this is a mistake\n",
    "            one_hot.scatter_(1, indices, 1)\n",
    "\n",
    "            # Construct output lists\n",
    "            output.append(probs.view(batch_size, 1, self.output_size))\n",
    "            preactivation.append(pre.view(batch_size, 1, self.hidden_size))\n",
    "            sampled_output.append(one_hot)\n",
    "\n",
    "            if groundTruth is not None:\n",
    "                # Teacher force actual ground-truth\n",
    "                target = groundTruth[:, i, :]\n",
    "            else:\n",
    "                # Feed in own prediction\n",
    "                target = one_hot\n",
    "        \n",
    "        output = torch.cat(output, 1) # log probabilites\n",
    "        preactivation = torch.cat(preactivation, 1)\n",
    "        sampled_output = torch.stack(sampled_output, 1)\n",
    "        \n",
    "        # output probabilities instead of log probs\n",
    "        return output, preactivation, sampled_output, hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# NEW MODEL\n",
    "model = VAE(INPUT_SIZE=input_size,\n",
    "            CHARSET_LEN=len(char_list)+1,\n",
    "            LATENT_DIM=292,\n",
    "            filter_sizes=(9,9,10),\n",
    "            kernel_sizes=(5,5,7),\n",
    "            eps_std=1e-2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.eval()\n",
    "\n",
    "x = X[:10]\n",
    "z = model.reparameterize(*model.encode(x))\n",
    "zout = F.selu(model.linear_3(z))\n",
    "zprime = zout.view(zout.size(0), 1, zout.size(-1)).repeat(1, model.INPUT_SIZE, 1)\n",
    "o, h = model.stacked_gru(zprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, preactivation, sampled_output, hx = teachergru.forward(o, groundTruth=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputt, preactivationt, sampled_outputt, hxt = terminalgru.forward(o, groundTruth=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_output == sampled_outputt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:, 0, :].data.new(output[:, 0, :].size(0), terminalgru.output_size).zero_().scatter_(1, topi, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.scatter_(1, indices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(output == outputt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachergru.output == terminalgru.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0], outputt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy(y_pred, y_true, batch_average=True, timestep_average=True):\n",
    "    # scale preds so that the class probas of each sample sum to 1\n",
    "    cumsum = torch.sum(y_pred, dim=-1)[:, :, None].repeat(1, 1, y_pred.size()[-1])  # need to repeat until we have keepdim from master\n",
    "    y_pred /= cumsum\n",
    "    # manual computation of crossentropy\n",
    "    epsilon = 1E-7\n",
    "    output = F.hardtanh(y_pred, min_val=epsilon, max_val=1. - epsilon)\n",
    "    loss = -torch.sum(y_true.detach() * torch.log(output))\n",
    "\n",
    "    if batch_average:\n",
    "        loss /= y_pred.size()[0]\n",
    "    if timestep_average:\n",
    "        loss /= y_pred.size()[1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.exp(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_crossentropy(probs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_shaped = probs.view(-1, probs.size(-1))\n",
    "x_shaped = x.view(-1, x.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(probs_shaped, x_shaped, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy(probs_shaped, x_shaped, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photocatalysis.deeplearning.models import cyclical_annealing, linear_annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "steps = [i for i in range(0, T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [cyclical_annealing(s, T, 4) for s in steps]\n",
    "betas_lin = [linear_annealing(s, T) for s in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, betas)\n",
    "plt.plot(steps, betas_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_z(z, noise_norm, num_samples=1, constant_norm=False):\n",
    "    assert z.ndim == 1, 'Can only process one latent vector z of shape 1xLATENT_DIM'\n",
    "    # Generate points that lie uniformly on a sphere centered at z of radius 'AT' or 'UPTO' noise_norm\n",
    "    # Draw a noise vector from std norm distb. and normalize it\n",
    "    z = np.tile(z, (num_samples, 1))\n",
    "    noise_vec = np.random.normal(0, 1, size=z.shape)\n",
    "    noise_vec /= np.linalg.norm(noise_vec, axis=1)[:, None]\n",
    "    if constant_norm:\n",
    "        # at noise_norm std deviations away... \n",
    "        return z + noise_norm * noise_vec\n",
    "    else:\n",
    "        # upto noise_norm std deviations away (draw multiple norms within the sphere R < noise_norm)\n",
    "        noise_amp = np.random.uniform(0, noise_norm, size=(z.shape[0], 1))\n",
    "        return z + noise_amp * noise_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.random.randn(292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Z = perturb_z(Z, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photocatalysis.deeplearning.models import VAE, train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training definitions\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 120\n",
    "\n",
    "# NEW MODEL\n",
    "model = VAE(INPUT_SIZE=input_size,\n",
    "            CHARSET_LEN=len(char_list),\n",
    "            LATENT_DIM=292,\n",
    "            filter_sizes=(5,5,6),\n",
    "            kernel_sizes=(5,5,7)).to(device)\n",
    "\n",
    "# LOAD PREV MODEL\n",
    "# LOAD PREV MODEL\n",
    "# model = VAE()\n",
    "# model.load_state_dict(torch.load('/content/drive/MyDrive/VAE_model_parmas.pt', map_location=torch.device(device)))\n",
    "# model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# torchinfo.summary(VAE(), input_size=(batch_size, 120, 33))\n",
    "# torchinfo.summary(model, input_size=(batch_size, input_size, len(char_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "tls, vls = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    training_losses, validation_loss = train_epoch(train_loader,\n",
    "                                                   model,\n",
    "                                                   optimizer,\n",
    "                                                   validation_data_loader=valid_loader,\n",
    "                                                   device=device,\n",
    "                                                   charset=char_list,\n",
    "                                                   epoch=epoch)\n",
    "    \n",
    "    tls.append(training_losses), vls.append(validation_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
